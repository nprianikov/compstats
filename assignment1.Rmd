---
title: "KEN4258: Computational Statistics"
subtitle: "Assignment 1"
author: 
  - Aur√©lien Bertrand
  - Bart van Gool
  - Gaspar Kuper
  - Ignacio Cadarso Quevedo
  - Nikola Prianikov
date: "February 2024"
output: pdf_document
classoption: a4paper
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE, 
  message = FALSE, 
  fig.width = 5, 
  fig.height = 4, 
  out.width = "50%", 
  fig.align = "center"
)
```

# Assignment 1

Link to our GitHub repository: https://github.com/nprianikov/compstats

## 1) Create a Monte Carlo simulation to illustrate the problem

The Monte Carlo simulation below is intended to show that the $R^2$ of a model will never decrease, even if predictors are added which don't have anything to do with the data. First data is generated by using $5$ predictors, and a linear model is fit to it. After this, $10$ random predictors are generated and added to the model. The plot at the bottom of Section 1 shows how the $R^2$ value continues to grow even though the predictors bare no actual relation to the data.

```{r,echo=FALSE}
# 1. First, we define the data and function for simulating:

simulate_R_squared_range <- function(n, p, m) {
  
  # Step 1: Generate original random feature matrix X and coefficients beta
  X <- matrix(rnorm(n * p), nrow = n, ncol = p)
  beta <- rnorm(p)
  y_true <- X %*% beta + rnorm(n)
  
  # Generate m additional predictors
  X_new <- matrix(rnorm(m * n), nrow = n, ncol = m)
  
  # Initialize a vector to store R^2 values
  r_squared_values <- numeric(m+1)
  
  model_extended <- lm(y_true ~ X - 1)
  r_squared_values[1] <- summary(model_extended)$r.squared
  
  for (i in 1:m) {
    # Step 2 & 3: Extend X by adding one more column from X_new at a time
    X_extended <- cbind(X, X_new[, 1:i])
    
    # Step 4 & 5: Fit a linear model using the extended X and y
    model_extended <- lm(y_true ~ X_extended - 1) # '- 1' to exclude intercept
    
    # Step 6: Calculate R^2 value for the fitted model
    r_squared_values[i+1] <- summary(model_extended)$r.squared
  }
  return(r_squared_values)
}

# Monte Carlo simulation
monte_carlo_simulation_range <- function(n, p, r, m) {
  # Replicate r times the simulation and store each set of m R^2 values
  simulations <- replicate(r, simulate_R_squared_range(n, p, m), simplify = "array")
  
  # Compute the mean R^2 over all simulations for each of the m values
  mean_R_squared <- apply(simulations, 1, mean)
  return(mean_R_squared)
}
```

```{r, echo=FALSE}
# 2. Experiment with different number of predictors and plot the $R^2$:

library(tibble)
library(ggplot2)

set.seed(42)

# Create data frame of the observed R^2 w.r.t. number of predictors
df_experiment <- tibble(
  predictors = seq(0, 10, by = 1),
  r_squared_values = monte_carlo_simulation_range(100, 5, 500, 10)
)

# Plot
ggplot(df_experiment, aes(x = predictors, y = r_squared_values)) +
  geom_line() +
  labs(title = "R-squared values vs. Number of Added Predictors",
       x = "Number of Added Predictors",
       y = "R-squared"
   ) +
  scale_x_continuous(breaks = seq(0, 10, by = 1))  
```
We can see that the $R^2$ increases with the number of predictors even though they have no relationship with the generated data. Although these random predictors can slightly increase the accuracy on the training data, they make make the true performance of the model worse. This suggests that $R^2$ does not reflect the true goodness (accuracy) of the fit.

## 2) Provide a mathematical proof showing that the problem really exists.
The $R^2$ score is given by

$$
R^2 = \frac{\text{Var}(X\hat{\beta})}{\text{Var}(Y)} 
    = \frac{\text{Var}(Y) - \text{Var}(e)}{\text{Var}(Y)} 
    = 1 - \frac{\text{Var}(e)}{\text{Var}(Y)}
$$

The $\text{Var}(e)$ depends on the the sum of squared residuals as the variance is defined as $\frac{1}{n} \sum_i {e_i^2}$. 

$$
e^{(1)}_i = Y_i - \alpha - X_i \beta_1
$$


## 3) Propose a solution to address the problem.
As shown in Section 2, the problem with $R^2$ is that it is highly influenced by the number of predictors used in the model. In particular, as the number of predictors grows, the $R^2$ score of the model can only increase or stay the same but not decrease. The issue is then that predictors can be included in the model which don't actually have any relationship to the data, but allow the model to gain some accuracy anyway. Although adding such predictors to the model may seem like a good idea when looking at the $R^2$ score, it can cause the model to overfit to the training data.

This drawback of the $R^2$ metric has been well documented, and a solution to it has been proposed in the adjusted $R^2$. The formula for the adjusted $R^2$ metric is the following:

$$
\text{Adjusted } R^2 = 1 - (1-R^2) \times \frac{n - 1}{n - p - 1} 
= 1 - \frac{\text{Var}(e)}{\text{Var}(Y)} \times \frac{n - 1}{n - p - 1}
$$

In the adjusted $R^2$ an extra term is introduced at the end which is a function of $n$ and $p$. Since $n$ is a constant, this term is only influenced by $p$. As $p$ gets larger, the denominator gets smaller, causing this term to increase. This means that as new predictors are introduced to the model, the adjusted $R^2$ metric will apply a penalty. Therefore, if a predictor does not add enough predictive power to the model to counteract this penalty, the adjusted $R^2$ score will go down. 

The adjusted $R^2$ score, by applying this penalty to unneeded predictors, discourages overfitting of models. This can come in use, for example when comparing models trained using different amounts of predictors. If model A is able to achieve the same accuracy as model B, but it uses less predictors to do so, then model A would be preferred by the adjusted $R^2$ while the regular $R^2$ would give both models the same score.

## 4) Find a real dataset to illustrate the problem and your fix.
<<<<<<< HEAD

Here, we used the mtcars dataset, which contains $32$ observations and $11$ numeric variables. We fit a linear regression model starting with $1$ predictor, then added predictors until all the variables were used. The goal was to investigate the impact of adding predictors to $R^2$ and adjusted $R^2$. 

```{r}
=======
```{r, echo=FALSE}
>>>>>>> 6ad77c5514bb456ace33702af186f237e1efa5e1
library(tidyverse)
library(ggplot2)
library(dplyr)
library(datasets)

data(mtcars)
head(mtcars)
```

```{r, echo=FALSE}
predictors <- setdiff(names(mtcars), 'mpg')
r_squares <- numeric(length(predictors))
adjusted_r_squares <- numeric(length(predictors))
  
for (i in 1:length(predictors)) {
    formula <- as.formula(paste('mpg', "~", paste(predictors[1:i], collapse = "+")))
    model <- lm(formula, data = mtcars)
    r_squares[i] <- summary(model)$r.squared
    adjusted_r_squares[i] <- summary(model)$adj.r.squared
  }
  
results <- data.frame(NumPredictors = 1:length(predictors), R2 = r_squares, AdjustedR2 = adjusted_r_squares)

# Plot the results
ggplot(results, aes(x = NumPredictors)) +
  geom_line(aes(y = R2, colour = "R^2")) +
  geom_line(aes(y = AdjustedR2, colour = "Adjusted R^2")) +
  labs(x = "Number of Predictors", y = "Value", title = "R^2 and Adjusted R^2 vs. Number of Predictors in mtcars Dataset") +
  scale_colour_manual("", 
                      breaks = c("R^2", "Adjusted R^2"),
                      values = c("R^2" = "blue", "Adjusted R^2" = "red"))
```

As we can see on the plot above, whenever adding a predictor, we notice either an increase or no change to $R^2$. However, adjusted $R^2$ sometimes decreases when a new predictor is added, meaning that the new model performs worse. This comes from the fact that adjusted $R^2$, unlike $R^2$, takes into account the number of predictors.

Using these two metrics together, we can argue that using $5$ predictors for the example above is sufficient, since adding the rest of the predictors does not improve the model, even though $R^2$ increases.
