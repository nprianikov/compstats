---
title: "KEN4258: Computational Statistics"
subtitle: "Assignment 1"
author: "names..."
date: "February 2024"
output: pdf_document
classoption: a4paper
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE, 
  message = FALSE, 
  fig.width = 5, 
  fig.height = 4, 
  out.width = "50%", 
  fig.align = "center"
)
```

TODO

# Assignment 1
## 1) Create a Monte Carlo simulation to illustrate the problem

The Monte Carlo simulation below shows that a linear model can "perfectly fit" a random noise for a high number of prediction being close to number of observations. This is entailed by the $R^2$ to be high, although it is implausible to perfectly fit complete random data.

1. First, we define the data and function for simulating:
```{r}

simulate_R_squared_range <- function(n, p, m) {
  # Step 1: Generate original random feature matrix X and coefficients beta
  X <- matrix(rnorm(n * p), nrow = n, ncol = p)
  beta <- rnorm(p)
  y_true <- X %*% beta + rnorm(n)
  # Generate m additional predictors
  X_new <- matrix(rnorm(m * n), nrow = n, ncol = m)
  # Initialize a vector to store R^2 values
  r_squared_values <- numeric(m+1)
  
  model_extended <- lm(y_true ~ X - 1)
  r_squared_values[1] <- summary(model_extended)$r.squared
  
  for (i in 1:m) {
    # Step 2 & 3: Extend X by adding one more column from X_new at a time
    X_extended <- cbind(X, X_new[, 1:i])
    
    # Step 4 & 5: Fit a linear model using the extended X and y
    model_extended <- lm(y_true ~ X_extended - 1) # '- 1' to exclude intercept
    
    # Step 6: Calculate R^2 value for the fitted model
    r_squared_values[i+1] <- summary(model_extended)$r.squared
  }
  
  return(r_squared_values)
}

# Monte Carlo simulation
monte_carlo_simulation_range <- function(n, p, r, m) {
  # Replicate r times the simulation and store each set of m R^2 values
  simulations <- replicate(r, simulate_R_squared_range(n, p, m), simplify = "array")
  
  # Compute the mean R^2 over all simulations for each of the m values
  mean_R_squared <- apply(simulations, 1, mean)
  
  return(mean_R_squared)
}
```

2. Experiment with different number of predictors and plot the $R^2$:
```{r}
library(tibble)
library(ggplot2)

set.seed(42)

# Create data frame of the observed R^2 w.r.t. number of predictors
df_experiment <- tibble(
  predictors = seq(0, 10, by = 1),
  r_squared_values = monte_carlo_simulation_range(100, 5, 500, 10)
)
```

```{r}
# Plot
ggplot(df_experiment, aes(x = predictors, y = r_squared_values)) +
  geom_line() +
  labs(title = "R-squared values vs. Number of Added Predictors",
       x = "Number of Added Predictors",
       y = "R-squared"
   ) +
  scale_x_continuous(breaks = seq(0, 10, by = 1))  
```
We can see that the $R^2$ increases with the number of predictors. This suggests that $R^2$ does not reflect the true goodness (accuracy) of the fit.

## 2) Provide a mathematical proof showing that the problem really exists.
$R^2$ is given by
$$
R^2 = \frac{\text{Var}(X\hat{\beta})}{\text{Var}(Y)}
$$
We know that 
$$
\text{Var}(Y) = \text{Var}(X\hat{\beta}) + \text{Var}(e)
$$
So, 
$$
\text{Var}(X\hat{\beta}) = \text{Var}(Y) - \text{Var}(e)
$$

Rewriting $R^2$, we have
$$
R^2 = \frac{\text{Var}(X\hat{\beta})}{\text{Var}(Y)} 
    = \frac{\text{Var}(Y) - \text{Var}(e)}{\text{Var}(Y)} 
    = 1 - \frac{\text{Var}(e)}{\text{Var}(Y)}
$$
The $\text{Var}(e)$ depends on the the sum of squared residuals as the variance is defined as $\frac{1}{n} \sum_i {e_i^2}$. 

$$
e^{(1)}_i = Y_i - \alpha - X_i \beta_1
$$


## 3) Propose a solution to address the problem.


## 4) Find a real dataset to illustrate the problem and your fix.


